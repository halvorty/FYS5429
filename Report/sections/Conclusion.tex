\section{Conclusion}

In this report I have presented how Transformer based text embedding models work, 
starting from the basics of Feed Forward Neural Networks.
Then building upon this with the Recurrent Neural Network architecture,
which has been important in the field of Natural Language Processing (NLP),
highlighting a couple of limitations which the Transformer architecture 
to some extent solves. In addition a mini experiment was 
conducted to train a simple FNN to classify news articles based on
text embedding of their titles. The text embedding chosen was 
from a open source model from the team mixedbread, called \textit{mxbai-embed-large-v1},
and the accuracy achieved was $88.36\%$.
